{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scriptspath = os.path.join(os.getcwd(), '..', 'scripts')\n",
    "sys.path.append(scriptspath)\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.core.lightning import LightningModule\n",
    "\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import wget\n",
    "\n",
    "import string\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download data\n",
    "pdb = '4rb6Y'  # other examples - 5cajA , 4x5lA\n",
    "a3m_url = 'https://files.ipd.uw.edu/krypton/%s.i90c75.a3m'%pdb\n",
    "pdb_url = 'https://files.ipd.uw.edu/krypton/%s.pdb'%pdb\n",
    "cf_url  = 'https://files.ipd.uw.edu/krypton/%s.cf'%pdb\n",
    "urls    = [a3m_url, pdb_url, cf_url]\n",
    "fnames  = [url.split('/')[-1] for url in urls]\n",
    "\n",
    "for i, url in enumerate(urls):\n",
    "    fname = fnames[i]\n",
    "    if not os.path.exists(fname):\n",
    "        wget.download(url=url, out=fname);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the MSA one hot representation, the sequence weights and the contacts\n",
    "a3mfname, _, cffname = fnames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdb_list = ['4rb6Y', '5cajA', '4x5lA']\n",
    "pdb = '4rb6Y'  # '5cajA' '4x5lA'  <-- 3 different datasets to play with\n",
    "X, W, cons = extract_data(pdb)\n",
    "N, L, A = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def parse_a3m(filename, a3m=False):\n",
    "#     '''function to parse fasta file'''\n",
    "\n",
    "#     if a3m:\n",
    "#         import string\n",
    "#         # for a3m files the lowercase letters are removed\n",
    "#         # as these do not align to the query sequence\n",
    "#         rm_lc = str.maketrans(dict.fromkeys(string.ascii_lowercase))\n",
    "    \n",
    "#     header, sequence = [],[]\n",
    "#     lines = open(filename, \"r\")\n",
    "#     for line in lines:\n",
    "#         line = line.rstrip()\n",
    "#         if line[0] == \">\":\n",
    "#             header.append(line[1:])\n",
    "#             sequence.append([])\n",
    "#         else:\n",
    "#             if a3m: line = line.translate(rm_lc)\n",
    "#             else: line = line.upper()\n",
    "#             sequence[-1].append(line)\n",
    "#     lines.close()\n",
    "#     sequence = [''.join(seq) for seq in sequence]\n",
    "\n",
    "#     return header, sequence\n",
    "\n",
    "# def create_msa(seqs, alphabet=\"ARNDCQEGHILKMFPSTWYV-\"):\n",
    "#     '''one hot encode msa'''\n",
    "#     states = len(alphabet)  \n",
    "\n",
    "#     # create dictionary of alphabet\n",
    "#     a2n = {a:n for n, a in enumerate(alphabet)}\n",
    "\n",
    "#     # get indices from dictionary for each AA at each position\n",
    "#     msa_ori = np.array([[a2n.get(aa, states-1) for aa in seq] for seq in seqs])\n",
    "\n",
    "#     # return one-hot\n",
    "#     return np.eye(states)[msa_ori]\n",
    "\n",
    "# def calculate_seq_weights(msa, eff_cutoff=0.8):\n",
    "#     '''compute weight per sequence'''\n",
    "#     if msa.ndim == 3: msa = msa.argmax(-1)    \n",
    "#     msa_sm = 1.0 - squareform(pdist(msa, \"hamming\"))\n",
    "#     weights = 1/(msa_sm >= eff_cutoff).astype(np.float).sum(-1)\n",
    "#     return weights\n",
    "\n",
    "# def parse_cf(filename, cutoff=0.001):\n",
    "#     # get contacts\n",
    "#     # contact Y,1     Y,2     0.006281        MET     ARG\n",
    "\n",
    "#     # parse contact file\n",
    "#     n, cons = 0, []\n",
    "#     for line in open(filename, \"r\"):\n",
    "#         line = line.rstrip()\n",
    "#         if line[:7] == \"contact\":\n",
    "#             _, _, i, _, j, p, _, _ = line.replace(\",\", \" \").split()\n",
    "#             i, j, p = int(i), int(j), float(p)\n",
    "#             if i > n: \n",
    "#                 n = i\n",
    "#             if j > n: \n",
    "#                 n = j\n",
    "#             cons.append([i-1, j-1, p])\n",
    "\n",
    "#     # create contact map\n",
    "#     cm = np.zeros([n, n])\n",
    "#     for i, j, p in cons: \n",
    "#         cm[i,j] = p\n",
    "#     return (cm + cm.T)\n",
    "\n",
    "\n",
    "# def extract_data(pdb):\n",
    "#     \"\"\"\n",
    "#     INPUTS:\n",
    "#         pdb <str> - The pdb code\n",
    "        \n",
    "#     RETURNS:\n",
    "#         1. msa <numpy.ndarray> - \n",
    "#         2. W <numpy.ndarray> -\n",
    "#         3. nat_contact <>\n",
    "#     \"\"\"\n",
    "#     names, seqs = parse_a3m(f\"{pdb}.i90c75.a3m\", a3m=True)  # parse sequences\n",
    "#     msa = create_msa(seqs)                # convert sequences to one-hot\n",
    "#     W = calculate_seq_weights(msa)        # weight per sequence \n",
    "#     nat_contacts = parse_cf(f\"{pdb}.cf\")  # get contacts from .cf file\n",
    "#     return msa, W, nat_contacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# An example sequence one hot encoding  \n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.spy(X[np.random.randint(0, len(X))].T)\n",
    "plt.xticks(np.arange(0, 107, 5), np.arange(0, 107, 5)+1)\n",
    "plt.yticks(np.arange(0, 21, 5), np.arange(0, 21, 5)+1)\n",
    "plt.xlabel('Sequence index', fontsize=15)\n",
    "plt.ylabel('Amino acid', fontsize=15)\n",
    "plt.grid(color='w', linewidth=1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSADataset(Dataset):\n",
    "    def __init__(self, X, W):\n",
    "        self.X = X  # shape (N, L, A)\n",
    "        self.W = W  # shape (N,)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        xbatch = self.X[idx]\n",
    "        wbatch = self.W[idx]\n",
    "        return torch.tensor(xbatch, dtype=torch.float32), \\\n",
    "                torch.tensor(wbatch, dtype=torch.float32)\n",
    "\n",
    "class Reshape(nn.Module):\n",
    "    def __init__(self, target_shape):\n",
    "        \"\"\"\n",
    "        The target shape to reshape incoming tensors to. \n",
    "\n",
    "        Ex.\n",
    "        >> x = torch.randn(10, 3, 2)\n",
    "        >> layer = Reshape((6,))\n",
    "        >> y = Reshape(x)     # shape == (10, 6)\n",
    "\n",
    "        \"\"\"\n",
    "        super(Reshape, self).__init__()\n",
    "        self.target_shape = list(target_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        xshape = list(x.shape)\n",
    "        yshape = [xshape[0]] + self.target_shape\n",
    "        y = x.reshape(*yshape)\n",
    "        return y\n",
    "\n",
    "class Permute(nn.Module):\n",
    "    def __init__(self, *dims):\n",
    "        super(Permute, self).__init__()\n",
    "        self.dims = dims\n",
    "    def forward(self, x):\n",
    "        assert x.ndim == len(self.dims)\n",
    "        return x.permute(*self.dims)\n",
    "    \n",
    "class GeneralLinear(nn.Module):\n",
    "    def __init__(self, units):\n",
    "        super(GeneralLinear, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.empty((units, units)))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.matmul(x, self.weight)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.xavier_uniform_(self.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LAE(LightningModule):\n",
    "    def __init__(self, N, L, A, \n",
    "                 latent_dim=32, \n",
    "                 lambda_w=0.1, \n",
    "                 lambda_e=1.0, \n",
    "                 use_e=True, \n",
    "                 use_bias=False,\n",
    "                 baselr=1e-3):\n",
    "        super(LAE, self).__init__()\n",
    "        self.L=L\n",
    "        self.A=A\n",
    "        self.baselr = baselr\n",
    "        self.lambda_w = lambda_w*L*A/N\n",
    "        self.lambda_e = lambda_e\n",
    "        F=L*A\n",
    "        \n",
    "        # encoder\n",
    "        flatten = nn.Flatten()\n",
    "        linear1 = nn.Linear(F, latent_dim, bias=use_bias)\n",
    "        self.encoder = nn.Sequential(*[flatten, linear1])\n",
    "        \n",
    "        # decoder \n",
    "        linear2 = nn.Linear(latent_dim, F)\n",
    "        reshape = Reshape((L, A))  \n",
    "        self.decoder = nn.Sequential(*[linear2, reshape])  # shape (N, L, A)\n",
    "        \n",
    "        # optional embedding \n",
    "        if use_e:\n",
    "            linear3 = GeneralLinear(A)\n",
    "        else:\n",
    "            linear3 = nn.Identity()\n",
    "        softmax = nn.Softmax(dim=2)\n",
    "        permute = Permute(0, 2, 1)  \n",
    "        self.output = nn.Sequential(*[linear3, softmax, permute])  # final shape (N, A, L)\n",
    "        \n",
    "        # cross entropy loss function \n",
    "        self.crossentropy = nn.CrossEntropyLoss(reduction='none')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.encoder(x)\n",
    "        y = self.decoder(y)\n",
    "        y = self.output(y)\n",
    "        return y\n",
    "    \n",
    "    def encoder_decoder_reg(self):\n",
    "        reg = torch.tensor(0.)\n",
    "        for m in self.encoder:\n",
    "            if hasattr(m, 'weight'):\n",
    "                reg += m.weight.norm()**2\n",
    "        for m in self.decoder:\n",
    "            if hasattr(m, 'weight'):\n",
    "                reg += m.weight.norm()**2\n",
    "        return self.lambda_w*reg\n",
    "    \n",
    "    def embedding_reg(self):\n",
    "        reg = torch.tensor(0.)\n",
    "        for m in self.output:\n",
    "            if hasattr(m, 'weight'):\n",
    "                reg += m.weight.norm()**2\n",
    "        return self.lambda_e*reg\n",
    "    \n",
    "    def lossfn(self, ypred, xs, ws):\n",
    "        crossentropy = (ws[:, None] * self.crossentropy(ypred, xs.argmax(dim=2))).mean()\n",
    "        reg = self.encoder_decoder_reg() + self.embedding_reg()\n",
    "        return crossentropy + reg\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        xbatch, wbatch = batch\n",
    "        loss = self.lossfn(self(xbatch), xbatch, wbatch)\n",
    "        \n",
    "        # return a dictionary of results \n",
    "        tensorboard_logs = {'train_loss':loss}\n",
    "        return {'loss':loss, 'log':tensorboard_logs}\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=self.baselr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator \n",
    "batch_size = 128\n",
    "dataset = MSADataset(X, W)\n",
    "dataloader = DataLoader(dataset=dataset, \n",
    "                        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# LAE model\n",
    "baselr = 0.1*np.log(W.sum())/L\n",
    "lae = LAE(N, L, A, latent_dim=512, baselr=baselr, lambda_w=1e-4, lambda_e=1e-4)\n",
    "\n",
    "# model checkpoint \n",
    "model_idx = 1\n",
    "cwd = os.getcwd()\n",
    "lae_dir = os.path.join(cwd, 'lae_results')\n",
    "checkpoint_dir = os.path.join(lae_dir, 'lae_%d'%model_idx)\n",
    "if not os.path.exists(checkpoint_dir):\n",
    "    os.makedirs(checkpoint_dir)\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(checkpoint_dir, monitor='train_loss')\n",
    "\n",
    "\n",
    "# train\n",
    "trainer_params = {\n",
    "    \"precision\":32,\n",
    "    \"max_epochs\":50,\n",
    "    \"checkpoint_callback\":checkpoint_callback\n",
    "}\n",
    "trainer = pl.Trainer(**trainer_params)\n",
    "trainer.fit(lae, dataloader);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best model checkpoint path \n",
    "bestcheckpoint = torch.load(checkpoint_callback.best_model_path)\n",
    "print(\"Best loss : %.5f\"%bestcheckpoint['checkpoint_callback_best_model_score'])\n",
    "beststatedict  = bestcheckpoint['state_dict']\n",
    "lae.load_state_dict(beststatedict)\n",
    "lae.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate pairwise saliency\n",
    "lae_w = pairwise_saliency(lae)\n",
    "\n",
    "# convert w matrix to contact map \n",
    "lae_contacts = pairwise_contact_map(lae_w)\n",
    "\n",
    "# plot\n",
    "fig = plot_contact_map(cons, lae_contacts, num_contacts=L, thresh=0.001, cutoff=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# contact accuracy \n",
    "fraction = np.linspace(0.1, 1.0, 10)\n",
    "eval_points = (fraction*L).astype(\"int\")\n",
    "lae_con_acc = contact_accuracy(lae_contacts, cons, eval_points)\n",
    "plt.plot(fraction, lae_con_acc, label=\"LAE\", c=\"red\", linewidth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for evaluating models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_saliency(model):\n",
    "    \"\"\"\n",
    "    Given the PyTorch model, compute the pairwise saliency.\n",
    "    \"\"\"\n",
    "    inputs = torch.zeros((A, L, A), requires_grad=True)\n",
    "    outputs = model(inputs)\n",
    "    pw = []\n",
    "    for j in range(L):\n",
    "        out_j    = outputs[:, :, j] # output at jth position \n",
    "        y = -torch.sum(torch.eye(A)*torch.log(out_j + 1e-8))\n",
    "        saliency = -torch.autograd.grad(y, inputs, retain_graph=True)[0]\n",
    "        pw.append(saliency.data.numpy())\n",
    "    pw = np.array(pw)\n",
    "    pw = 0.5*(pw + np.transpose(pw, (2, 3, 0, 1)))\n",
    "    return pw\n",
    "\n",
    "def apc(x, rm_diag=True):\n",
    "    \"\"\"\n",
    "    Given contact map do APC (average product correction)\n",
    "    \"\"\"\n",
    "    if rm_diag:\n",
    "        np.fill_diagonal(x, 0.)\n",
    "    a1 = x.sum(0, keepdims=True)\n",
    "    a2 = x.sum(1, keepdims=True)\n",
    "    y =  x - (a1*a2) / x.sum()\n",
    "    np.fill_diagonal(y, 0.)\n",
    "    return y\n",
    "\n",
    "def pairwise_contact_map(pw, do_apc=True):\n",
    "    \"\"\"\n",
    "    Given pairwise term, i.e., W matrix (L, A, L, A), compute \n",
    "    contact map\n",
    "    \"\"\"\n",
    "    l2_norm = np.sqrt(np.square(pw[:, :20, :, :20]).sum((1,3)))\n",
    "    if do_apc:\n",
    "        return apc(l2_norm)\n",
    "    else:\n",
    "        return l2_norm\n",
    "\n",
    "def contact_accuracy(pred, meas, eval_points, thresh=0.01, cutoff=6):\n",
    "    \"\"\"\n",
    "    compute agreement between predicted and measured contact map\n",
    "    INPUTS:\n",
    "        1. pred <numpy.ndarray> - predicted contact map\n",
    "        2. meas <numpy.ndarray> - measured contact map, i.e. from pdb structure\n",
    "        3. eval_points <numpy.ndarray> - number of contacts to consider for accuracy calculation\n",
    "        4. thresh <float> - 0.01, <-- if above threshold then considered a contact\n",
    "        5. cutoff <float> - distance off-diagonal to consider for contact accuracy\n",
    "    \"\"\"\n",
    "\n",
    "    # get rid of close contacts up to cutoff\n",
    "    eval_idx = np.triu_indices_from(meas, cutoff)\n",
    "    pred_, meas_ = pred[eval_idx], meas[eval_idx] \n",
    "\n",
    "    # get highest predictions\n",
    "    sort_idx = np.argsort(pred_)[::-1] \n",
    "\n",
    "    # calculate true positives for top L predictions\n",
    "    return [(meas_[sort_idx[:l]] > thresh).mean() for l in eval_points]\n",
    "\n",
    "def plot_contact_map(nat_contacts, pred_contacts, num_contacts, thresh=0.01, cutoff=6):\n",
    "\n",
    "    fig = plt.figure(figsize=(5, 5))\n",
    "\n",
    "    # plot natural contacts that are off-diagonal greater than cutoff and greater than threhsold\n",
    "    triu_idx = np.triu_indices_from(nat_contacts, cutoff)\n",
    "    vals = nat_contacts[triu_idx]\n",
    "    vals_sort_idx = vals > thresh\n",
    "    plt.scatter(triu_idx[0][vals_sort_idx], triu_idx[1][vals_sort_idx], c='lightgray', s=30)\n",
    "    plt.scatter(triu_idx[1][vals_sort_idx], triu_idx[0][vals_sort_idx], c='lightgray', s=30)\n",
    "\n",
    "    # get indices of off-diagonal contacts to evaluate \n",
    "    # (contacts less than this is too easy -- usually secondary structures)\n",
    "    eval_idx = np.triu_indices_from(pred_contacts, cutoff)\n",
    "    pred_, meas_ = pred_contacts[eval_idx], nat_contacts[eval_idx] \n",
    "\n",
    "    # get the top num_contacts predictions\n",
    "    sort_idx = np.argsort(pred_)[::-1][:num_contacts]\n",
    "\n",
    "    # plot correct contacts in blue\n",
    "    good_index = sort_idx[meas_[sort_idx] > thresh]\n",
    "    plt.scatter(eval_idx[0][good_index], eval_idx[1][good_index], c='blue', s=2)\n",
    "    plt.scatter(eval_idx[1][good_index], eval_idx[0][good_index], c='blue', s=2)\n",
    "\n",
    "    # plot incorrect contacts in blue\n",
    "    bad_index = sort_idx[meas_[sort_idx] <= thresh]\n",
    "    plt.scatter(eval_idx[0][bad_index], eval_idx[1][bad_index], c='red', s=2)\n",
    "    plt.scatter(eval_idx[1][bad_index], eval_idx[0][bad_index], c='red', s=2)\n",
    "\n",
    "    # clean up plot\n",
    "    L = len(nat_contacts)\n",
    "    plt.xlim(0, L)\n",
    "    plt.ylim(0, L)\n",
    "    plt.gca().set_ylim(plt.gca().get_ylim()[::-1])\n",
    "    plt.axis('tight')\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
